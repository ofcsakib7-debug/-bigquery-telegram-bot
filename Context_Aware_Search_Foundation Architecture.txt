Design 6, Phase 3: BQML-Powered Context-Aware Search - Validation & Auto-Correction Prompt for Qwen Coder
Create the data validation and auto-correction system for the BQML-powered context-aware search that operates entirely within Google Cloud free tier limits while providing department-specific typo correction and intent refinement. Implement exactly as specified with no interpretation or deviation.
Core Validation & Correction Architecture
1. Multi-Layered Validation Funnel
Exact Implementation Requirements:
python
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
⌄
⌄
⌄
⌄
⌄
⌄
⌄
def validate_search_query(user_id, query_text):
    # LAYER 1: Syntax Validation (ALWAYS first, 0 quota cost)
    if not validate_syntax(query_text):
        return {
            'status': 'REJECTED',
            'error_type': 'SYNTAX',
            'error_message': "Invalid characters. Use only letters, numbers, and spaces."
        }
    
    # LAYER 2: Logical Validation (ALWAYS second, 0 quota cost)
    logical_validation = validate_logic(user_id, query_text)
    if not logical_validation['valid']:
        return {
            'status': 'REJECTED',
            'error_type': 'LOGIC',
            'error_message': logical_validation['message'],
            'suggestions': logical_validation['suggestions']
        }
    
    # LAYER 3: Heuristic Pattern Check (ALWAYS third, 0 quota cost)
    heuristic_result = check_heuristic_patterns(user_id, query_text)
    if heuristic_result['suspicious']:
        # LAYER 4: Semantic Validation (ONLY for suspicious queries)
        semantic_result = validate_semantically(user_id, query_text, heuristic_result)
        return semantic_result
    
    # Query passed all validation layers
    return {
        'status': 'APPROVED',
        'query_text': query_text,
        'confidence_score': heuristic_result['confidence_score']
    }
Critical Implementation Rules:
•	LAYER 1 MUST complete within 5ms (regex validation only)
•	LAYER 2 MUST complete within 10ms (simple logic checks)
•	LAYER 3 MUST complete within 50ms (BQML-powered pattern check)
•	NEVER call LAYER 4 unless LAYER 3 flags query as suspicious
•	ALWAYS return validation results within 100ms of request
•	NEVER use NL API or Vision API - all validation must be quota-free
2. Syntax Validation Layer (Layer 1)
Exact Implementation Requirements:
python
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
⌄
⌄
⌄
⌄
⌄
def validate_syntax(query_text):
    """Layer 1: Basic syntax validation (0 quota cost)"""
    # Check character set (only allowed characters)
    if not re.match(r'^[a-z0-9\s{}]+$', query_text):
        return False
    
    # Check length constraints
    if len(query_text) < 2 or len(query_text) > 20:
        return False
    
    # Check variable format (if present)
    if '{' in query_text or '}' in query_text:
        if not re.match(r'^[^{}]*({[a-z]+}[^{}]*)*$', query_text):
            return False
    
    return True
Critical Implementation Rules:
•	ONLY allow lowercase letters, numbers, spaces, and {variables}
•	Variable format must be {name} (e.g., {name}, {id}, {time})
•	Query length must be 2-20 characters
•	Syntax validation must use regex only (NO external calls)
•	MUST reject queries with uppercase letters or special characters
•	MUST return specific error message for each validation failure
3. Logical Validation Layer (Layer 2)
Exact Implementation Requirements:
python
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
⌄
⌄
⌄
⌄
⌄
⌄
⌄
⌄
⌄
def validate_logic(user_id, query_text):
    """Layer 2: Department-specific logic validation (0 quota cost)"""
    department = get_user_department(user_id)
    
    # Check for valid department-specific patterns
    valid_patterns = get_department_patterns(department)
    
    # Check if query matches any valid pattern structure
    pattern_match = None
    for pattern in valid_patterns:
        if re.fullmatch(pattern['regex_pattern'], query_text):
            pattern_match = pattern
            break
    
    if not pattern_match:
        # Find closest matching patterns for suggestions
        suggestions = find_closest_patterns(query_text, valid_patterns)
        return {
            'valid': False,
            'message': "Invalid search pattern for your department.",
            'suggestions': suggestions
        }
    
    # Check variable constraints (if applicable)
    variables = extract_variables(query_text, pattern_match['pattern'])
    variable_errors = []
    
    for var_name, var_value in variables.items():
        if var_name == 'time' and var_value not in ['cm', 'lm', 'lw', 'tw', 'ly']:
            variable_errors.append(f"'{var_value}' is not a valid time period")
        # Additional variable validation rules...
    
    if variable_errors:
        return {
            'valid': False,
            'message': "Invalid parameters: " + ", ".join(variable_errors),
            'suggestions': generate_variable_suggestions(variables, pattern_match)
        }
    
    return {'valid': True}
Critical Implementation Rules:
•	Department-specific pattern validation:
•	ACCOUNTING: Must validate payment, voucher, expense patterns
•	MARKETING: Must validate visit, lead, customer patterns
•	INVENTORY: Must validate stock, delivery, service patterns
•	SERVICE: Must validate ticket, machine, parts patterns
•	SALES: Must validate order, quotation, performance patterns
•	HR: Must validate payroll, leave, staff patterns
•	MANAGEMENT: Must validate revenue, profit, KPI patterns
•	Time period validation rules:
•	cm = current month
•	lm = last month
•	lw = last week
•	tw = this week
•	ly = last year
•	Variable validation must be department-specific
•	MUST return specific suggestions for invalid patterns
4. Heuristic Pattern Check (Layer 3)
Exact Implementation Requirements:
python
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
⌄
⌄
⌄
⌄
⌄
⌄
⌄
⌄
def check_heuristic_patterns(user_id, query_text):
    """Layer 3: BQML-powered pattern analysis (0 quota cost)"""
    # 1. FIRST: Check master_cache for instant response
    cached_result = bigquery.query("""
        SELECT suspicion_score, confidence_score 
        FROM `project.dataset.master_cache`
        WHERE 
          cache_key = 'heuristic:{user_id}:{query_text}'
          AND expires_at > CURRENT_TIMESTAMP()
        LIMIT 1
    """, params={'user_id': user_id, 'query_text': query_text})
    
    if cached_result:
        return {
            'suspicious': cached_result[0]['suspicion_score'] > 0.3,
            'confidence_score': cached_result[0]['confidence_score']
        }
    
    # 2. If cache miss, use BQML model for prediction
    prediction = bigquery.query("""
        SELECT 
          suspicion_score,
          confidence_score
        FROM ML.PREDICT(
          MODEL `project.dataset.heuristic_model`,
          STRUCT(
            @query_text AS input_text,
            @user_id AS user_id,
            FORMAT_TIMESTAMP('%H', CURRENT_TIMESTAMP()) AS hour_of_day,
            FORMAT_TIMESTAMP('%A', CURRENT_TIMESTAMP()) AS day_of_week
          )
        )
        LIMIT 1
    """, params={'query_text': query_text, 'user_id': user_id})
    
    # 3. Cache the prediction for future use
    bigquery.insert('master_cache', [{
        'cache_key': f'heuristic:{user_id}:{query_text}',
        'cached_data': json.dumps({
            'suspicion_score': prediction[0]['suspicion_score'],
            'confidence_score': prediction[0]['confidence_score']
        }),
        'expires_at': datetime.now() + timedelta(hours=1),
        'created_at': datetime.now()
    }])
    
    return {
        'suspicious': prediction[0]['suspicion_score'] > 0.3,
        'confidence_score': prediction[0]['confidence_score']
    }
Critical Implementation Rules:
•	suspicion_score threshold: > 0.3 = suspicious (requires deeper validation)
•	ALWAYS check master_cache first for instant response
•	NEVER query raw data in user-facing requests
•	ALWAYS filter by partitioning column first in all queries
•	ALWAYS include at least one clustering column in WHERE clause
•	Use approximate functions where exact counts aren't needed
•	NEVER use SELECT * - always specify exact columns needed
5. Typo Correction System
Exact Implementation Requirements:
python
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
⌄
⌄
⌄
⌄
⌄
def find_typo_corrections(user_id, query_text):
    """Find possible typo corrections using common patterns"""
    department = get_user_department(user_id)
    
    # 1. FIRST: Check common_corrections cache
    corrections = bigquery.query("""
        SELECT 
            original_text,
            corrected_text,
            confidence_score
        FROM `project.dataset.common_corrections`
        WHERE 
          department_id = @department
          AND original_text LIKE CONCAT('%', @query_text, '%')
        ORDER BY confidence_score DESC
        LIMIT 3
    """, params={'department': department, 'query_text': query_text})
    
    if corrections:
        return corrections
    
    # 2. If cache miss, use BQML to predict corrections
    return predict_corrections(department, query_text)

def predict_corrections(department, query_text):
    """Use BQML to predict possible corrections"""
    return bigquery.query("""
        SELECT 
            original_text,
            corrected_text,
            confidence_score
        FROM ML.PREDICT(
          MODEL `project.dataset.typo_correction_model`,
          STRUCT(
            @query_text AS input_text,
            @department AS department_id
          )
        )
        WHERE confidence_score > 0.4
        ORDER BY confidence_score DESC
        LIMIT 3
    """, params={'query_text': query_text, 'department': department})
Critical Implementation Rules:
•	common_corrections table structure:
sql
1
2
3
4
5
6
7
8
9
10
11
12
13
14
⌄
CREATE TABLE `project.dataset.common_corrections` (
  correction_id STRING NOT NULL,
  department_id STRING NOT NULL,
  original_text STRING NOT NULL,
  corrected_text STRING NOT NULL,
  levenshtein_distance INT64 NOT NULL,
  usage_count INT64 DEFAULT 1,
  last_used TIMESTAMP,
  confidence_score FLOAT64 NOT NULL,
  created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP()
)
PARTITION BY DATE(created_at)
CLUSTER BY department_id, levenshtein_distance
OPTIONS(description="Common typo corrections with usage tracking");
•	levenshtein_distance must be calculated as:
sql
1
SELECT `project.dataset.levenshtein_distance`(original_text, corrected_text) AS distance
•	confidence_score must be between 0.0 and 1.0
•	ONLY return corrections with confidence_score > 0.4
•	ALWAYS limit to 3 corrections max
6. Auto-Correction Training System
Exact Implementation Requirements:
sql
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
⌄
⌄
-- Create training data for typo correction
CREATE OR REPLACE TABLE `project.dataset.bqml_training_typo_correction` AS
SELECT
  original_text,
  corrected_text,
  levenshtein_distance,
  department_id,
  usage_count,
  confidence_score,
  CURRENT_DATE() AS training_date
FROM `project.dataset.common_corrections`
WHERE DATE(created_at) >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY);

-- Create typo correction model
CREATE OR REPLACE MODEL `project.dataset.typo_correction_model`
OPTIONS(
  model_type = 'kmeans',
  num_clusters = 10
) AS
SELECT
  original_text,
  corrected_text,
  levenshtein_distance,
  department_id
FROM `project.dataset.bqml_training_typo_correction`;
Critical Implementation Rules:
•	Training data must only include corrections from last 90 days
•	Model must use KMEANS clustering with 10 clusters
•	levenshtein_distance must be included as a feature
•	department_id must be included as a categorical feature
•	Model must be rebuilt daily during off-peak hours (2AM-4AM Bangladesh time)
•	Training data must be partitioned by DATE(created_at)
•	Training data must be clustered by department_id
7. Department-Specific Validation Rules
Accounting Department Rules:
sql
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
⌄
⌄
-- Single character validation
SELECT 
  'e' AS pattern, '^[a-z0-9\s{}]*$' AS regex_pattern, 'expenses' AS description
UNION ALL
SELECT 'p', '^[a-z0-9\s{}]*$', 'payments'
UNION ALL
SELECT 'v', '^[a-z0-9\s{}]*$', 'vouchers'
UNION ALL
SELECT 'b', '^[a-z0-9\s{}]*$', 'bank reconciliation'
UNION ALL
SELECT 'r', '^[a-z0-9\s{}]*$', 'revenue report';

-- Double character validation
SELECT 
  'cm' AS pattern, '^(cm|lm|ly|lw|tw)$' AS regex_pattern, 'time period' AS description
UNION ALL
SELECT 'pd', '^(pd|pa|un|vp)$', 'payment status'
UNION ALL
SELECT 'vp', '^(vp|vs|hi)$', 'voucher status';
Critical Implementation Rules:
•	Time period patterns must match exactly: cm, lm, ly, lw, tw
•	Payment status patterns must match exactly: pd, pa, un, vp
•	Voucher status patterns must match exactly: vp, vs, hi
•	Department-specific patterns must be loaded into memory cache
•	NEVER query validation rules directly during search processing
•	ALWAYS use pre-loaded validation rules from memory cache
8. Validation Audit Workflow
Exact Implementation Requirements:
sql
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
⌄
CREATE TABLE `project.dataset.validation_audit_workflow` (
  audit_id STRING NOT NULL,  -- Format: AUDIT-{YYYYMMDD}-{3-random}
  user_id STRING NOT NULL,
  department_id STRING NOT NULL,
  raw_input STRING NOT NULL,
  input_type STRING NOT NULL,  -- Values: SEARCH_QUERY, SHORTCUT, CHALLAN, etc.
  validation_status STRING NOT NULL,  -- Values: APPROVED, REJECTED, CORRECTED
  final_validation_status STRING NOT NULL,  -- Values: APPROVED, REJECTED, CORRECTED
  auditor_id STRING,  -- Telegram ID of the user who approved a correction
  corrected_value STRING,  -- The final, correct value after audit
  auto_suggestion_applied BOOL NOT NULL,  -- True if the system's auto-correction was accepted
  timestamp TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP()
)
PARTITION BY DATE(timestamp)
CLUSTER BY user_id, input_type, validation_status
OPTIONS(
  description="Audit workflow for validation and corrections with BQML integration"
);
Critical Implementation Rules:
•	audit_id must follow exact format: AUDIT-20231109-ABC
•	validation_status must be one of 3 specified values only
•	final_validation_status must be one of 3 specified values only
•	auto_suggestion_applied must be BOOL (TRUE/FALSE)
•	timestamp must use UTC timezone
•	Implement data expiration: 36 months after timestamp
•	Partitioning must be on DATE(timestamp), NOT _PARTITIONTIME
•	Clustering must include user_id as first field
9. Common Corrections Cache
Exact Implementation Requirements:
sql
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
⌄
CREATE TABLE `project.dataset.common_corrections` (
  correction_id STRING NOT NULL,
  department_id STRING NOT NULL,
  original_text STRING NOT NULL,
  corrected_text STRING NOT NULL,
  levenshtein_distance INT64 NOT NULL,
  usage_count INT64 DEFAULT 1,
  last_used TIMESTAMP,
  confidence_score FLOAT64 NOT NULL,
  created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP()
)
PARTITION BY DATE(created_at)
CLUSTER BY department_id, levenshtein_distance
OPTIONS(
  description="Common typo corrections with usage tracking for auto-correction"
);
Critical Implementation Rules:
•	correction_id must follow exact format: CORR-20231109-ABC
•	levenshtein_distance must be between 1 and 20
•	confidence_score must be between 0.0 and 1.0
•	usage_count must increment on each successful correction
•	last_used must update on each successful correction
•	Implement automatic cleanup of low-confidence corrections
•	Partitioning must be on DATE(created_at), NOT created_at
10. Validation & Correction Processing Flow
Accounting Department Example: "a2b" Query:
sql
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
⌄
⌄
⌄
-- LAYER 1: Syntax validation (passes)
-- LAYER 2: Logical validation (passes - matches machine model pattern)
-- LAYER 3: Heuristic pattern check
SELECT suspicion_score, confidence_score
FROM ML.PREDICT(
  MODEL `project.dataset.heuristic_model`,
  STRUCT(
    'a2b' AS input_text,
    '12345' AS user_id,
    '10' AS hour_of_day,
    'WEDNESDAY' AS day_of_week
  )
);

-- Result: suspicion_score = 0.25, confidence_score = 0.85
-- Query NOT suspicious - no deeper validation needed

-- Check for typo corrections
SELECT 
  original_text,
  corrected_text,
  confidence_score
FROM `project.dataset.common_corrections`
WHERE 
  department_id = 'ACCOUNTING'
  AND original_text LIKE '%a2b%'
ORDER BY confidence_score DESC
LIMIT 1;

-- Result: No corrections found

-- Process query as normal
SELECT 
  machine_model_name,
  current_stock,
  dealer_price,
  bottom_price
FROM `project.dataset.machine_inventory_cache`
WHERE 
  machine_model_id = 'A2B'
  AND branch_id = (SELECT branch_id FROM user_profiles WHERE user_id = '12345')
LIMIT 1;
Critical Implementation Rules:
•	If query is "a2b cm" (suspicion_score = 0.45), show alternatives:
•	"a2b lw" for last week's sales? (42% confidence)
•	"a2b cm" for current month sales report? (38% confidence)
•	"a2b p" for parts for A2B machine? (25% confidence)
•	If query is "a2b" with typo (e.g., "a2c"), suggest "Did you mean 'a2b'?" with 85% confidence
•	ALWAYS return primary results within 1 second
•	ALWAYS return alternatives within 3 seconds of primary results
•	NEVER show more than 3 alternatives
Critical Implementation Requirements
1. Validation Funnel Guarantees
•	LAYER 1: Must complete within 5ms with 100% accuracy
•	LAYER 2: Must complete within 10ms with 95% accuracy
•	LAYER 3: Must complete within 50ms with 85% accuracy
•	LAYER 4: Must only be called for 10-15% of queries
•	ALWAYS return validation results within 100ms of request
•	NEVER use NL API or Vision API - all validation must be quota-free
2. Department-Specific Validation Rules
•	ACCOUNTING: Prioritize numeric patterns and date references
•	MARKETING: Prioritize name and location patterns
•	INVENTORY: Prioritize machine model and branch patterns
•	SERVICE: Prioritize ticket ID and machine patterns
•	SALES: Prioritize customer and order patterns
•	HR: Prioritize staff ID and date patterns
•	MANAGEMENT: Prioritize financial metric patterns
3. Typo Correction Implementation
•	MUST use levenshtein_distance for text similarity
•	MUST calculate confidence_score based on usage_count and levenshtein_distance
•	MUST limit to 3 corrections max
•	MUST update usage_count on successful correction
•	MUST automatically learn from validation_audit_workflow
4. Quota-Saving Implementation Techniques
•	Micro-Batching: Collect validation results before writing to cache
•	Aggressive Caching: 90% of validation checks should come from cache
•	Partition Filtering: Always filter by DATE(timestamp) first
•	Clustering Utilization: Always include department_id as first clustering column
•	Data Minimization: Only select necessary columns, never SELECT *
•	Approximate Functions: Use APPROX_COUNT_DISTINCT where exact counts aren't needed
5. Error Handling Requirements
•	NEVER show technical error messages to users
•	ALWAYS provide actionable next steps
•	ALWAYS fall back to cached results if validation fails
•	ALWAYS log validation errors to validation_audit_workflow
•	NEVER allow validation processing to exceed 100ms
This validation and auto-correction system must operate entirely within Google Cloud free tier limits while providing department-specific typo correction and intent refinement. Pay special attention to partitioning and clustering strategies to minimize data scanned per query.
DO NOT IMPLEMENT ANY ADDITIONAL VALIDATION LAYERS BEYOND WHAT IS SPECIFIED ABOVE.
The system must be designed so that 85% of search queries pass through LAYER 3 without needing deeper validation, and 95% of typo corrections come from the common_corrections cache.
This is the complete Phase 3 specification. Implement exactly as specified without interpretation or deviation.

